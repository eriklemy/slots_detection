{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection Model Training & Evaluation\n",
        "\n",
        "**Author:** Erick Lemmy dos Santos Oliveira\n",
        "\n",
        "**GitHub:** [Slots Detection](https://github.com/eriklemy/slots_detection)\n",
        "\n",
        "**Technical Report:** Full execution workflow and experimental results from Google Colaboratory, including training/evaluation metrics and model performance visualizations [[PDF]](https://github.com/eriklemy/slots_detection/blob/main/notebook/home_task.pdf)\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This project implements an object detection system to detect Slots for planting flowers. Despite the limited dataset size, various techniques where tested to create a detection model capable of performing effectively.\n",
        "\n",
        "### Project Objectives\n",
        "1. Train an object detection model with the data (Yolo or any other model).\n",
        "2. Evaluate the model performance (select and justify evaluation metrics).\n",
        "3. Discuss the model performance and errors.\n",
        "4. Discuss the steps used to solve the problem.\n",
        "5. Suggest improvements in data/model/etc.\n",
        "\n",
        "### Dataset Characteristics\n",
        "- Classes: 1 (Slots)\n",
        "- Total images: 21\n",
        "- split:\n",
        "    - train: 16 images (75%)\n",
        "    - val: 5 images    (25%)\n",
        "\n",
        "### Technical Environment\n",
        "- Hardware: Google Colab GPU (NVIDIA T4/V100)\n",
        "- Framework: Ultralytics YOLO\n",
        "- Acceleration: CUDA-enabled training\n"
      ],
      "metadata": {
        "id": "3RGD0bwrBsPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDRXU_ogBnSQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics sahi albumentations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the necessary imports bellow"
      ],
      "metadata": {
        "id": "k4uupnPNB5ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "cKZfx5vkB7HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify NVIDIA GPU Availability**\n",
        "\n",
        "Checking for the avaliability of the GPU. If shows its off, go to \"Runtime\" -> \"Change Runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section."
      ],
      "metadata": {
        "id": "JEI6-JltB_Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = 0 if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "xVlJm7TwCCIw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp; Dataset Verification\n",
        "\n"
      ],
      "metadata": {
        "id": "7T5x42uRCEhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Uploading the **dataset.zip** file to the Google Colab instance by clicking the \"Files\" icon on the left side of the browser, and then the \"Upload to session storage\" icon. Select the zip folder to upload it or upload from your personal Google Drive, mount the drive on this Colab session, and copy them over to the Colab filesystem. For my dataset is **erick.zip** you can change to use another, but it will require to update every instance of **erick** in the code.\n",
        "\n"
      ],
      "metadata": {
        "id": "MTFvKFsmBIuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/erick.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "ecNt7sGfMKSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Pre-Training Dataset Evaluation**  \n",
        "\n",
        "Before training, it is essential to evaluate the dataset by verifying that both images and labels are correct. This process also helps in identifying a suitable model based on the dataset format, reducing the need for significant modifications if it already aligns with a specific model.  \n",
        "\n",
        "### **Reasons:**  \n",
        "\n",
        "1. **Prevent Training Failures**  \n",
        "   - YOLO coordinates outside the [0,1] range can cause numerical errors (e.g., NaN loss).  \n",
        "   - Incorrect folder structure can prevent data from loading properly.  \n",
        "\n",
        "2. **Ensure Annotation Quality**  \n",
        "   - Poor annotations can reduce mAP scores.  \n",
        "   - Unlabeled objects introduce bias, leading to inaccurate learning.  \n",
        "\n",
        "3. **YOLO Compatibility**  \n",
        "   - Requires a specific directory format:  \n",
        "     ```bash\n",
        "     dataset/\n",
        "     ├── train/images/  # .jpg, .png\n",
        "     └── train/labels/  # .txt (one file per image)\n",
        "     ```  \n",
        "   - Class IDs must match the YAML configuration file.  \n",
        "\n",
        "The provided dataset follows the standard structure, with separate training and validation sets, as well as distinct folders for images and labels. Since it includes a YAML file and annotations in `.txt` format structured as:  \n",
        "   - (`class`, `center_x`, `center_y`, `height`, `width`)  \n",
        "   - Values normalized between **0 and 1**  \n",
        "\n",
        "This suggests that the dataset was created using an auxiliary labeling tool, such as [Roboflow](https://roboflow.com), and later converted to YOLO format. By visualizing an image alongside its corresponding annotations, the format was confirmed.**negrito**"
      ],
      "metadata": {
        "id": "hkj5riGvCMBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_label_check(img, title):\n",
        "    sample_img = cv2.imread(img_path)\n",
        "    annotated_img = sample_img.copy()\n",
        "\n",
        "    h, w = sample_img.shape[:2]\n",
        "    label_path = img_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
        "\n",
        "            x_center, y_center = int(x_center * w), int(y_center * h)\n",
        "            width, height = int(width * w), int(height * h)\n",
        "\n",
        "            x1, y1 = x_center - width // 2, y_center - height // 2\n",
        "            x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "            cv2.rectangle(annotated_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax[0].imshow(cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB))\n",
        "    ax[0].set_title(\"Original Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "\n",
        "    ax[1].imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
        "    ax[1].set_title(\"Annotated Image\")\n",
        "    ax[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f\"{title}: {os.path.basename(img_path)}\")\n",
        "    plt.show()\n",
        "\n",
        "img_path = \"erick/val/images/7126_18_7281_71-6_2ND.jpg\"\n",
        "plot_label_check(img_path, \"Sample Image with Bounding Box\")"
      ],
      "metadata": {
        "id": "qcHEFiG3CPWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Image Resolution and Color Analysis"
      ],
      "metadata": {
        "id": "ZhDAfDPcCVC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizing Image Resolutions**  \n",
        "\n",
        "Analyzing the distribution of image resolutions in the **training** and **validation** sets helps identify potential imbalances that could affect model training and generalization.  \n",
        "\n",
        "### **Methodology:**  \n",
        "1. **Extract Resolutions:** Collect width and height from all images.  \n",
        "2. **Count Occurrences:** Determine the number of unique resolutions and the number of images per resolution.  \n",
        "\n",
        "### **Observations:**  \n",
        "\n",
        "1. **Training Set Imbalance:**  \n",
        "   - 80% of the training images are concentrated in just **two resolutions** (`4000x2252` and `4032x3024`).  \n",
        "   - Resolutions like `1600x900` and `2000x1126` contain only **one image each**, which could lead to *overfitting* for these specific formats.  \n",
        "\n",
        "2. **Limited Validation Coverage:**  \n",
        "   - The validation set covers only **50%** of the resolutions found in the training set.  \n",
        "   - The resolution `4032x3024` has only **one image** in validation, making evaluation unreliable for this format.  \n",
        "\n",
        "### **Implications:**  \n",
        "- **Resolution Bias:** The model may perform poorly on underrepresented resolutions (e.g., `1600x900`).  \n",
        "- **Preprocessing Needs:** Different resolutions require strategies like *resizing* or *padding* for consistency.  \n",
        "- **Generalization Issues:** The validation set does not sufficiently cover the training resolutions, increasing performance drops in real-world deployment."
      ],
      "metadata": {
        "id": "AC66xwziCXCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resolution_and_color_means(image_dir):\n",
        "    resolutions = []\n",
        "    color_means = []\n",
        "\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is not None:\n",
        "            # Get resolution\n",
        "            h, w = img.shape[:2]\n",
        "            resolutions.append((w, h))\n",
        "\n",
        "            # Calculate color means (BGR channels)\n",
        "            color_means.append(img.mean(axis=(0, 1)))\n",
        "    return resolutions, color_means\n",
        "\n",
        "train_image_dir = \"erick/train/images\"\n",
        "train_label_dir = \"erick/train/labels\"\n",
        "\n",
        "val_image_dir = \"erick/val/images\"\n",
        "val_label_dir = \"erick/val/images\"\n",
        "\n",
        "resolutions_train, color_means_train = get_resolution_and_color_means(train_image_dir)\n",
        "resolutions_val, color_means_val = get_resolution_and_color_means(val_image_dir)\n",
        "\n",
        "print(f\"========= Resolutions =========\")\n",
        "print(f\"Number of unique resolutions (train): {len(set(resolutions_train))}\")\n",
        "print(f\"Images per resolution (train): {dict((res, resolutions_train.count(res)) for res in set(resolutions_train))}\")\n",
        "print(f\"Number of unique resolutions (val): {len(set(resolutions_val))}\")\n",
        "print(f\"Images per resolution (val): {dict((res, resolutions_val.count(res)) for res in set(resolutions_val))}\")"
      ],
      "metadata": {
        "id": "bAVEegKhCZKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze Color Channels\n",
        "\n",
        "Comparing the statistical distributions (mean and standard deviation) of the color channel intensities (BGR) between the training and validation sets to ensure consistency and identifying potential biases that may affect the model.\n",
        "\n",
        "### **Methodology:**  \n",
        "1. **Calculation of Statistics:**  \n",
        "   - Mean and standard deviation of the average intensities for each channel (Blue, Green, Red) across all images.  \n",
        "2. **Visualization:**  \n",
        "   - Histograms to compare intensity distributions.  \n",
        "   - Boxplots to analyze variation and outliers.  \n",
        "\n",
        "### **Observations:**\n",
        "The original dataset does not exhibit many outliers and appears consistent with the expected scenario. The high Green and Red values are due to the presence of plants and ground textures, while Blue is typically less prominent in natural environments. As a result, there is no significant indication of bias. However, applying a Hue augmentation could help simulate greater color variability and further enhance the model's robustness.\n"
      ],
      "metadata": {
        "id": "8y7p2ROuCe3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_means_train = np.array(color_means_train)\n",
        "color_means_val = np.array(color_means_val)\n",
        "\n",
        "print(f\"========= Color Channel Statistics =========\")\n",
        "print(f\"Color means -> B: {color_means_train[:, 0].mean()}, G: {color_means_train[:, 1].mean()}, R: {color_means_train[:, 2].mean()}\")\n",
        "print(f\"Color std -> B: {color_means_train[:, 0].std()}, G: {color_means_train[:, 1].std()}, R: {color_means_train[:, 2].std()}\")"
      ],
      "metadata": {
        "id": "O3UqVqPSCgv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_color_analysis(color_means, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot histogram\n",
        "    ax1.hist(color_means[:, 0], bins=50, alpha=0.5, color='b', label='Blue')\n",
        "    ax1.hist(color_means[:, 1], bins=50, alpha=0.5, color='g', label='Green')\n",
        "    ax1.hist(color_means[:, 2], bins=50, alpha=0.5, color='r', label='Red')\n",
        "    ax1.set_title(f'Color Histogram - {title}')\n",
        "    ax1.set_xlabel('Pixel Intensity Mean')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot boxplot\n",
        "    ax2.boxplot([color_means[:, 0], color_means[:, 1], color_means[:, 2]])\n",
        "    ax2.set_title(f'Color Channel Distributions - {title}')\n",
        "    ax2.set_xticks([1, 2, 3])\n",
        "    ax2.set_xticklabels(['Blue', 'Green', 'Red'])\n",
        "    ax2.set_ylabel('Pixel Intensity Mean')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_color_analysis(color_means_train, 'Train Dataset')\n",
        "# plot_color_analysis(color_means_val, 'Validation Dataset')"
      ],
      "metadata": {
        "id": "TGHslcCVCjIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataset_statistics(image_dir, label_dir):\n",
        "    \"\"\"Analyze dataset statistics including bounding box metrics and class distribution.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing computed statistics\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'bbox_areas': [],\n",
        "        'bbox_aspect_ratios': [],\n",
        "        'class_counts': defaultdict(int),\n",
        "        'num_boxes_per_image': [],\n",
        "        'missing_images': 0,\n",
        "        'corrupted_files': 0\n",
        "    }\n",
        "\n",
        "    label_files = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    print(f\"Analyzing {len(label_files)} label files...\")\n",
        "\n",
        "    for label_file in tqdm(label_files, desc=\"Processing images\"):\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        image_file = os.path.splitext(label_file)[0] + \".jpg\"\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "\n",
        "        # Validate image existence\n",
        "        if not os.path.exists(image_path):\n",
        "            stats['missing_images'] += 1\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            stats['corrupted_files'] += 1\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        num_boxes = 0\n",
        "\n",
        "        # Process labels\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                class_id, x_center, y_center, width, height = map(float, line.split())\n",
        "                stats['class_counts'][int(class_id)] += 1\n",
        "\n",
        "                # Convert to absolute coordinates\n",
        "                width_abs = width * w\n",
        "                height_abs = height * h\n",
        "\n",
        "                # Calculate metrics\n",
        "                area = width_abs * height_abs\n",
        "                aspect_ratio = width_abs / height_abs if height_abs > 0 else 0\n",
        "\n",
        "                stats['bbox_areas'].append(area)\n",
        "                stats['bbox_aspect_ratios'].append(aspect_ratio)\n",
        "                num_boxes += 1\n",
        "\n",
        "        stats['num_boxes_per_image'].append(num_boxes)\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_statistics(stats):\n",
        "    \"\"\"Print formatted statistics\"\"\"\n",
        "    total_images = len(stats['num_boxes_per_image'])\n",
        "    total_boxes = sum(stats['class_counts'].values())\n",
        "\n",
        "    print(\"\\nDataset Statistics Report\")\n",
        "    print(\"============================\")\n",
        "    print(f\"Total images analyzed: {total_images:,}\")\n",
        "    print(f\"Total bounding boxes: {total_boxes:,}\")\n",
        "    print(f\"Missing images: {stats['missing_images']}\")\n",
        "    print(f\"files: {stats['corrupted_files']}\\n\")\n",
        "\n",
        "    print(\"Bounding Box Metrics\")\n",
        "    print(f\"- Average boxes per image: {np.mean(stats['num_boxes_per_image']):.2f}\")\n",
        "    print(f\"- Area (pixels²) - Mean: {np.mean(stats['bbox_areas']):,.2f}, Median: {np.median(stats['bbox_areas']):,.2f}\")\n",
        "    print(f\"- Aspect Ratio - Mean: {np.mean(stats['bbox_aspect_ratios']):.2f}, Median: {np.median(stats['bbox_aspect_ratios']):.2f}\\n\")\n",
        "\n",
        "    print(\"Class Distribution\")\n",
        "    for class_id, count in sorted(stats['class_counts'].items()):\n",
        "        print(f\"  Class {class_id}: {count:,} boxes ({count/total_boxes:.1%})\")\n",
        "\n",
        "def plot_distributions(stats):\n",
        "    \"\"\"Visualize dataset distributions\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Bbox Area\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(stats['bbox_areas'], bins=30, color='skyblue', edgecolor='black')\n",
        "    plt.title('Bounding Box Area Distribution')\n",
        "    plt.xlabel('Pixels²')\n",
        "    plt.ylabel('Count')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    # Aspect Ratio\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(stats['bbox_aspect_ratios'], bins=30, color='salmon', edgecolor='black')\n",
        "    plt.title('Aspect Ratio Distribution')\n",
        "    plt.xlabel('Width/Height Ratio')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "train_stats = analyze_dataset_statistics(train_image_dir, train_label_dir)\n",
        "print_statistics(train_stats)\n",
        "plot_distributions(train_stats)"
      ],
      "metadata": {
        "id": "azMM_39TClWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.&nbsp;Train Model"
      ],
      "metadata": {
        "id": "xQGKCFy4Cr_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why YOLO?\n",
        "\n",
        "The YOLO architecture was selected for its:\n",
        "\n",
        "- Real-time inference capabilities\n",
        "- Balance between accuracy and speed\n",
        "- Strong performance on small datasets\n",
        "- Extensive pre-trained weights availability\n",
        "- Dataset Compatibility\n",
        "- Agricultural Applications [[1](https://www.sciencedirect.com/science/article/pii/S2772375524002533?via%3Dihub), [2](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5090169), [3](https://onlinelibrary.wiley.com/doi/10.1155/2023/4039179), [4](https://www.mdpi.com/2073-4395/14/10/2194), [5](https://www.mdpi.com/2073-4395/14/8/1628)]\n",
        "- SAHI compatibility        [[1](https://ieeexplore.ieee.org/document/9897990), [2](https://github.com/obss/sahi?tab=readme-ov-file)]\n",
        "- Augmentation capabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "4QzDUgatCsvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training Parameters\n",
        "With the model defined, the training starts! First, there are a few important parameters to decide on.\n",
        "\n",
        "**Model architecture & size (`model`):**\n",
        "\n",
        "Several YOLO versions exists, like yolov5, yolov8 and yolov11 with different models sizes (`yolov8n.pt`, `yolo8m.pt`, `yolo11n.pt`, `yolo11m.pt`, `yolo11x.pt`). Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. For this project the `yolov8n.pt` was chosen since it has less params than older models and provides balanced trade-off between speed and accuracy compared to others models like v9 (more computational heavy) and YOLO11 with is optimized for speed and efficiency, is not the best choice for this task since its focus is on detection smaller objects in higher resolution imagens. The decision to use the `n` variant was made to minimize/prevent overfitting during the transfer learning while ensuring that the model remains sufficiently capable of handling the complexity of small object detection.\n",
        "\n",
        "**Model architecture & size (`model`):**  \n",
        "\n",
        "Several YOLO versions exist, such as YOLOv5, YOLOv8, and YOLOv11, each offering models of varying sizes (`yolov8n.pt`, `yolov8m.pt`, `yolo11n.pt`, `yolo11m.pt`, `yolo11x.pt`). Larger models tend to run slower but offer higher accuracy, while smaller models run faster but sacrifice some accuracy. For this project, the `yolov8n.pt` model was selected. Although it has fewer parameters than the newer models (except YOLOv11), YOLOv8 provides a balanced trade-off between speed and accuracy. YOLOv11, while optimized for speed and efficiency, is not the best choice for this task since its focus is on maximizing performance in real-time scenarios, which is not the primary requirement here. YOLOv8, on the other hand, is well-suited for achieving good detection accuracy with reasonable processing time, making it ideal for the dataset and the computational constraints of this project.\n",
        "\n",
        "\n",
        "**Number of epochs (`epochs`) and (`batchs`)**\n",
        "\n",
        "With a limited dataset, an initial setting of 50 epochs is used to provide sufficient training while mitigating the risk of overfitting. YOLO automatically adjusts the batch size based on GPU memory, typically utilizing around 60% of the GPU’s RAM. For a resolution of 640x640, this usually results in a batch size between 8 and 16. This configuration strikes a balance between efficient training and resource utilization.\n",
        "\n",
        "\n",
        "**Resolution (`imgsz`)**\n",
        "\n",
        "Image Resolution has a large impact on the speed and accuracy of the mode. Lower resolution model will have higher speed but less accuracy. YOLO models are typically trained and inferenced at a 640x640 resolution.\n",
        "\n",
        "\n",
        "**Data Augmentation**\n",
        "\n",
        "Given the limited size of our dataset, we apply data augmentation to improve the model's generalization and robustness. YOLO provides built-in augmentation tools that eliminate the need to create entirely new datasets.\n",
        "The following augmentation techniques are in use:\n",
        "- Mosaic (1.0) – Set to 1.0 to ensure that all images will have that augmentation. This augmentation combines multiple images into one, helping the model learn object variations and different backgrounds.\n",
        "- MixUp (0.5) – Set to 0.5 to ensure that this augmentation only happens 50% of the time. This augmentation blends two images and their labels, improving the model's ability to handle occlusions and uncertainties reducing overfitting.\n",
        "- Scale (0.5) – Randomly resizes objects to enhance scale invariance. (50% probability to scale)\n",
        "- Flip (fliplr: 0.5, flipud: 0.2) – Applies horizontal and vertical flips to introduce viewpoint variations, horizontal is particularly effective for overhead/aerial images like those in our datase. (Aplied to 50% and 20% of images)\n",
        "- HSV Augmentation (hsv_h: 0.015, hsv_s: 0.7, hsv_v: 0.4) – Slightly hye Adjust to prevent extreme color shift, high saturation, and medium brightness to improve color invariance.\n",
        "\n",
        "\n",
        "**Transfer Learning**\n",
        "\n",
        "Using a pre-trained model leverages knowledge acquired from large datasets, facilitating the extraction of relevant features even if the target class isn't present in the original model. Freezing some of the initial layers preserves fundamental features, while the upper layers are fine-tuned to learn the new classes and adapt to our specific domain."
      ],
      "metadata": {
        "id": "oposNKGfDEx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Run the following code block to begin training."
      ],
      "metadata": {
        "id": "0ewzNT89DMv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf slots/*\n",
        "import yaml\n",
        "\n",
        "# Change to Absolute Path /content/\n",
        "data_yaml_path = \"/content/erick/data.yaml\"\n",
        "\n",
        "with open(data_yaml_path, 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "# Update val and train paths to absolute inside data.yaml\n",
        "data['train'] = '/content/erick/train/images'\n",
        "data['val'] = '/content/erick/val/images'\n",
        "\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    yaml.dump(data, f, default_flow_style=False)\n",
        "\n",
        "config = {\n",
        "    \"data\": data_yaml_path,\n",
        "    \"epochs\": 50,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch\": 16,\n",
        "    \"workers\": 8,      # Number of CPU workers (default: 8)\n",
        "    \"device\": device,\n",
        "    \"freeze\": list(range(11)), # 0 ... 10\n",
        "    \"mosaic\": 1.0,\n",
        "    \"mixup\": 0.5,\n",
        "    \"scale\": 0.5,\n",
        "    \"fliplr\": 0.5,\n",
        "    \"flipud\": 0.2,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.9,\n",
        "    \"hsv_v\": 0.9,\n",
        "    \"patience\": 30,     # for early stop if theres no improvement\n",
        "    \"seed\": 42,         # for reproducibility\n",
        "    # \"single_cls\": True, # improve, but changes the class name to 'item'\n",
        "    \"project\": \"slots\",\n",
        "    \"name\": \"standard\",\n",
        "}\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "results = model.train(**config)"
      ],
      "metadata": {
        "id": "1D4DuyipDOix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "To evaluate the model's performance, the metrics **Precision**, **Recall**, **mAP** (Mean Average Precision), and **mAP@50-95** were selected. These metrics are widely used in object detection evaluations and are standard in frameworks like YOLO, as well as in renowned benchmarks such as COCO.\n",
        "\n",
        "**Precision** and **Recall** provide essential insights into the quality of the detections by indicating the proportion of true positives relative to predictions and the model's ability to correctly identify objects. In contrast, **mAP@50-95** assesses the average precision of the model across different IoU (Intersection over Union) thresholds, offering a comprehensive view of detection accuracy at varying levels of overlap between predictions and ground truth objects."
      ],
      "metadata": {
        "id": "GdJmTH2qDTde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = YOLO(f\"{config['project']}/{config['name']}/weights/best.pt\")\n",
        "\n",
        "metrics = best_model.val()\n",
        "print(\"======== Metrics ========\")\n",
        "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
        "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
        "print(f\"Results: {metrics.box.p.mean():.4f}\")\n",
        "print(f\"Recall: {metrics.box.r.mean():.4f}\")"
      ],
      "metadata": {
        "id": "v03QQl_qDZ5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Results"
      ],
      "metadata": {
        "id": "wM93EfzpDcVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_path = Path(f\"{config['project']}/{config['name']}/\")\n",
        "results = cv2.imread(str(results_path / \"results.png\"))\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(cv2.cvtColor(results, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEwp_hVADbdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Image"
      ],
      "metadata": {
        "id": "hiQ1ocMjDpzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img = \"/content/erick/val/images/7039_14_151-6_2nd.jpg\"\n",
        "results = best_model(sample_img)\n",
        "\n",
        "res_plotted = results[0].plot(line_width=3)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title(\"Detected Objects\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kImWpELzDsMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Results Discution\n",
        "\n",
        "The initial results indicate that the model is learning, although there are signs of instability in both classification and bounding box predictions, as evidenced by the slight fluctuations in box_loss and cls_loss. While these losses are generally decreasing, the fact that precision and recall metrics jump to 1 early, while the model didnt fully converged yet, this suggests that the model is likely to be memorizing instead of actually learning.\n",
        "Additionally, when visualizing model outputs, there appears to be a wide range of predicted confidences (from about 0.2 up to 0.7) in certain areas of the image. This high variance could indicate that the model is unsure about specific regions."
      ],
      "metadata": {
        "id": "Zn2FMtqK4is_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.&nbsp;Improvement Strategy"
      ],
      "metadata": {
        "id": "N2MbyN4wD07I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing Strategy for Improving Small Object Detection\n",
        "\n",
        "The original images had high resolutions (e.g., 4032x3024, 4000x2252), and the objects to be detected were very small relative to the overall image size:  \n",
        "- **Average BBox area:** 18,936.92 pixels²  \n",
        "- **Median BBox area:** 20,804.56 pixels²  \n",
        "\n",
        "For reference, in a typical 4000x2252 image (~9MP):  \n",
        "- The object occupied only **~0.2% of the total image area** (18,936 / 9,000,000 ≈ 0.0021).\n",
        "\n",
        "### Identified Problem  \n",
        "At such high resolutions, the object becomes **nearly imperceptible** to conventional detection models, even when using absolute bounding boxes. This results in:  \n",
        "1. Loss of detail during downsampling (resizing to standard resolutions such as 640x640).  \n",
        "2. Learning difficulties due to the low pixel density of the object.\n",
        "\n",
        "### Implemented Solution  \n",
        "The images were divided into **640x640 pixel patches** using a slicing technique, resulting in:  \n",
        "- **Expansion of the training dataset:** from 16 to 581 images (train) and 5 to 197 (val).  \n",
        "- **Benefits:**  \n",
        "  - **Artificial increase in the object's relative resolution:**  \n",
        "    In a 640x640 patch (409,600 pixels²), the object's area now represents **~4.6%** (18,936 / 409,600) — making it 22 times more prominent.  \n",
        "  - Reduced computational complexity, as there are fewer pixels to process per iteration.  \n",
        "  - Generation of pseudo-augmented data that provides contextual variations.\n",
        "\n",
        "### Using SAHI (Slicing Aided Hyper Inference) for Small Objects\n",
        "\n",
        "#### How SAHI Complements YOLO\n",
        "| **Approach**         | **SAHI Advantage**                   | **Limitation of Pure YOLO**      |\n",
        "|----------------------|--------------------------------------|----------------------------------|\n",
        "| **Inference**        | Processes the image in slices        | Processes the entire image       |\n",
        "| **Object Size**      | Detects objects smaller than 50px    | Often misses smaller objects |\n",
        "| **Computational Cost** | Increases processing time by ~20%, but improves mAP@50-95 by ~30% | Faster inference, but with lower precision |\n",
        "\n",
        "This slicing strategy, especially when paired with SAHI, significantly enhances the detection of small objects by increasing their relative size and preserving more details during inference."
      ],
      "metadata": {
        "id": "-_6RW4mtDvHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation with Albumentations\n",
        "\n",
        "The use of the Albumentations library, in conjunction with YOLO augmentations, allows for the generation of a larger and more robust dataset by providing techniques not present in YOLO or with limited configuration options. In this sense, the following augmentations were applied to the pipeline:\n",
        "\n",
        "\n",
        "  ```python\n",
        "    # Augmentation pipeline\n",
        "    aug_transforms = A.Compose([\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.3),\n",
        "        A.HueSaturationValue(p=0.2),\n",
        "        A.RandomGamma(gamma_limit=(60, 115), p=0.3),\n",
        "        A.GaussNoise(p=0.3),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "  ```\n",
        "\n",
        "The levels of brightness, saturation, hue and GaussNoise were kept at their default values and adjusting just the probability to happen to , as they are already tuned for use with YOLO, while the RandomGamma required fine-tuning to better distribute the irregular exposure generated (dark/light). The use of Albumentations allowed for a 3x increase in the dataset size, totaling 1,940 images, with 1,743 for training and 197 for validation.\n",
        "This process can be seen in [`utils.py`](https://raw.githubusercontent.com/eriklemy/slots_detection/refs/heads/main/utils.py)\n"
      ],
      "metadata": {
        "id": "hTcY-DzrGKeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils File\n",
        "Run the block bellow to download the utils file (from the git repository) and perform the slicing and augmentation of the dataset"
      ],
      "metadata": {
        "id": "_0DUhVDmFbIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget -O /content/utils.py https://raw.githubusercontent.com/eriklemy/slots_detection/refs/heads/main/utils.py\n",
        "\n",
        "# remove the folder if need to remake the augmentation and slicing\n",
        "# !rm -r /content/sahi_augmented\n",
        "!python utils.py --data /content/erick/data.yaml --output /content/sahi_augmented --slice-size 640 --overlap-ratio 0.2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-_dh5zZgEcwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "config = {\n",
        "    \"data\": \"/content/sahi_augmented/data.yaml\",\n",
        "    \"epochs\": 50,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch\": 16,\n",
        "    \"workers\": 8,\n",
        "    \"device\": device,\n",
        "    \"freeze\": list(range(11)),\n",
        "    # \"augment\": True,   #  allow TTA\n",
        "    \"mosaic\": 1.0,       # reducing can improve performance\n",
        "    \"mixup\": 0.5,\n",
        "    \"scale\": 0.5,\n",
        "    \"fliplr\": 0.5,\n",
        "    \"flipud\": 0.2,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.7,\n",
        "    \"hsv_v\": 0.4,\n",
        "    \"patience\": 30,\n",
        "    \"seed\": 42,\n",
        "    # \"single_cls\": True, # improve, but change the class name to 'item'\n",
        "    \"project\": \"slots\",\n",
        "    \"name\": \"sahi\",\n",
        "}"
      ],
      "metadata": {
        "id": "dIccitc6Ey-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Run the block bellow to train the new version"
      ],
      "metadata": {
        "id": "IFRAvyweEwvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/slots/sahi\n",
        "\n",
        "results = model.train(**config)"
      ],
      "metadata": {
        "id": "vdLHmifkE51z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.&nbsp;Results Discution\n"
      ],
      "metadata": {
        "id": "P0OCjptJbNYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure below, which presents the model metrics, the training losses (`box_loss`, `cls_loss`, and `df_loss`) consistently decrease over the epochs, indicating that the model is effectively learning to locate and classify objects. The validation losses (`val/box_loss`, `val/cls_loss`, `val/df_loss`) also decline, albeit at a slightly slower pace, suggesting that is trying to generalize but the difference between train/val cls_loss and df_loss can imply overfitting.\n",
        "\n",
        "Meanwhile, the performance metrics (`Precision`, `Recall`, `mAP@50`, `mAP@50-95`) progressively increase and stabilize at high values indicating that the model is improving in terms of precision (correct predictions). However, the `mAP@50-95` still shows a drop, which suggest difficulties in precisely localizing objects at higher IoU thresholds."
      ],
      "metadata": {
        "id": "Hz-T75lDE9zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "\n",
        "results_path = Path(f\"/content/{config['project']}/{config['name']}/\")\n",
        "results = cv2.imread(str(results_path / \"results.png\"))\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(cv2.cvtColor(results, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# slicing over the original val (5 images - full size)\n",
        "utils.evaluate_with_sahi(f\"/content/{config['project']}/{config['name']}/weights/best.pt\", data_yaml_path)"
      ],
      "metadata": {
        "id": "67Uy8II1E_Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen in the figure bellow (after running the block) and in the folder **out/sahi** and **out/standard** (for the full quality image), applying SAHI significantly improved class identification accuracy, increasing between 15% and 20%. Additionally, detections became more consistent, reducing false positives and further demonstrating the effectiveness of the technique. However, it was observed that images with better lighting conditions yielded more stable results, suggesting that illumination plays a crucial role in detection performance and can be further improved.\n",
        "\n",
        "## Problems seen\n",
        "\n",
        "1. Aspect Ratio Dependency:\n",
        "Images resized without preserving their original aspect ratio (e.g., stretched or distorted) led to degraded performance. Analysis of the training data revealed that a large amount of annotations has a 1:1 aspect ratio for the slots (see aspect ratio distribution plot). This indicates that the model relies heavily on this ratio for slot detection, limiting its generalizability to non-square inputs.\n",
        "\n",
        "2. Lighting Sensitivity:\n",
        "Detection stability was significantly influenced by illumination quality, with poorly lit images yielding inconsistent results."
      ],
      "metadata": {
        "id": "TVvoM78KFIP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_dir = \"sahi_augmented/train/images\"\n",
        "label_image_dir = \"sahi_augmented/train/labels\"\n",
        "stats = analyze_dataset_statistics(train_image_dir, label_image_dir)\n",
        "plot_distributions(stats)"
      ],
      "metadata": {
        "id": "Bk-uUH8BNkl2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SAHI results\n",
        "sahi_image_paths = glob.glob(os.path.join(\"out/sahi/\", \"*.*\"))\n",
        "sahi_image_paths = [p for p in sahi_image_paths if p.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "sahi_img = cv2.imread(sahi_image_paths[0])\n",
        "\n",
        "# Load the standard YOLO results\n",
        "stantard_image_paths = glob.glob(os.path.join(\"out/standard/\", \"*.*\"))\n",
        "stantard_image_paths = [p for p in stantard_image_paths if p.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "stantard_img = cv2.imread(stantard_image_paths[0])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "axes[0].imshow(cv2.cvtColor(sahi_img, cv2.COLOR_BGR2RGB))\n",
        "axes[0].set_title(f\"SAHI: {os.path.basename(sahi_image_paths[0])}\", fontsize=12)\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(cv2.cvtColor(stantard_img, cv2.COLOR_BGR2RGB))\n",
        "axes[1].set_title(f\"Standard YOLO: {os.path.basename(stantard_image_paths[0])}\", fontsize=12)\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(f\"SAHI vs Standard YOLO Inference Comparison\", y=0.98)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5T_BYAfXFFnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"erick/val/images/7039_14_151-6_2nd.jpg\")\n",
        "img_resized = cv2.resize(img, (640, 640))\n",
        "model = YOLO(f\"{config['project']}/{config['name']}/weights/best.pt\")\n",
        "results = model(img_resized, verbose=False)\n",
        "res_plotted = results[0].plot(line_width=1)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title(\"Detected Objects in Resized Image\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oWwxxrfSW3Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Further Improvement Sugestions\n",
        "1. **Data Improvements**:\n",
        "   - Increase dataset size\n",
        "   - Add more challenging scenarios (occlusions, varying lighting)\n",
        "   - Improve annotation quality to reduce label incosistence\n",
        "2. **Model Improvements**:\n",
        "   - Medium YOLO variants (yolov8s/yolov8s/yolo11s/yolo11l), because Larger can cause overfitting if the dataset still small\n",
        "   - Explore R-CNN models for higher accuracy (like detectron2)\n",
        "   - Attention mecanism (models like RT-DET)\n",
        "3. **Training Improvements**:\n",
        "   - Hyperparameter optimization (learning rate, weight decay, optimizer like SGD can perform better than AdamW in some cases)\n",
        "   - Tunning with a wide space range (Computational Heavy)\n",
        "   - Longer training with progressive resizing (640, 1280, 2200)\n",
        "4. **Post-processing**:\n",
        "   - Optimize confidence thresholds to reduce false positives\n",
        "   - Implement/Use test-time augmentation (TTA)"
      ],
      "metadata": {
        "id": "gmsO5vJWNnNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "NZFFKEZvsTfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simplified CLI version for training, slicing, and augmentation is available in the **[Slots Detection](https://github.com/eriklemy/slots_detection)** repository.  \n",
        "\n",
        "**Usage Examples**:  \n",
        "1. **Full pipeline** (slice + augment + train):  \n",
        "   ```bash  \n",
        "   python main.py --data /path/to/data.yaml --output /path/output --slice-size 640 --overlap-ratio 0.2  \n",
        "   ```  \n",
        "\n",
        "2. **Skip augmentations** (slice + train):  \n",
        "   ```bash  \n",
        "   python main.py --data /path/to/data.yaml --output /path/output --slice-size 640 --overlap-ratio 0.2 --skip-augmentations  \n",
        "   ```  \n",
        "\n",
        "3. **Skip slicing/augmentation** (train only):  \n",
        "   ```bash  \n",
        "   python main.py --data /path/to/data.yaml --output /path/output --slice-size 640 --overlap-ratio 0.2 --skip-augmentations --skip-slicing  \n",
        "   ```  \n",
        "\n",
        "**Defaults**:  \n",
        "- `batch-size`: 8  \n",
        "- `epochs`: 100  \n",
        "- `img-size`: 640  \n",
        "- `model-size`: n  \n",
        "\n",
        "*For custom augmentations or advanced parameters, modify the configuration directly in the script.*  \n",
        "\n",
        "Install dependencies if needed\n",
        "```bash  \n",
        "pip install -r requirements.txt\n",
        "```  \n",
        "---"
      ],
      "metadata": {
        "id": "4Px-iLOmsY6V"
      }
    }
  ]
}