{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RGD0bwrBsPd"
      },
      "source": [
        "# Object Detection Model Training & Evaluation\n",
        "\n",
        "**Author:** Erick Lemmy dos Santos Oliveira\n",
        "\n",
        "**GitHub:** [Slots Detection](https://github.com/eriklemy/slots_detection)\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This project implements an object detection system to detect Slots for planting flowers. Despite the limited dataset size, various techniques where tested to create a detection model capable of performing effectively.\n",
        "\n",
        "### Project Objectives\n",
        "1. Train an object detection model with the data (Yolo or any other model).\n",
        "2. Evaluate the model performance (select and justify evaluation metrics).\n",
        "3. Discuss the model performance and errors.\n",
        "4. Discuss the steps used to solve the problem.\n",
        "5. Suggest improvements in data/model/etc.\n",
        "\n",
        "### Dataset Characteristics\n",
        "- Classes: 1 (Slots)\n",
        "- Total images: 21\n",
        "- split:\n",
        "    - train: 16 images (75%)\n",
        "    - val: 5 images    (25%)\n",
        "\n",
        "### Technical Environment\n",
        "- Hardware: Google Colab GPU (NVIDIA T4/V100)\n",
        "- Framework: Ultralytics YOLO\n",
        "- Acceleration: CUDA-enabled training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDRXU_ogBnSQ"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics sahi albumentations\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4uupnPNB5ML"
      },
      "source": [
        "Run the necessary imports bellow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKZfx5vkB7HW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEI6-JltB_Pe"
      },
      "source": [
        "**Verify NVIDIA GPU Availability**\n",
        "\n",
        "Checking for the avaliability of the GPU. If shows its off, go to \"Runtime\" -> \"Change Runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVlJm7TwCCIw"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = 0 if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5x42uRCEhR"
      },
      "source": [
        "#1.&nbsp; Dataset Verification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecNt7sGfMKSV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/erick.zip\" -d \"/content/datasets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkj5riGvCMBG"
      },
      "source": [
        "\n",
        "### **Pre-Training Dataset Evaluation**  \n",
        "\n",
        "Before training, it is essential to evaluate the dataset by verifying that both images and labels are correct. This process also helps in identifying a suitable model based on the dataset format, reducing the need for significant modifications if it already aligns with a specific model.  \n",
        "\n",
        "### **Reasons:**  \n",
        "\n",
        "1. **Prevent Training Failures**  \n",
        "   - YOLO coordinates outside the [0,1] range can cause numerical errors (e.g., NaN loss).  \n",
        "   - Incorrect folder structure can prevent data from loading properly.  \n",
        "\n",
        "2. **Ensure Annotation Quality**  \n",
        "   - Poor annotations can reduce mAP scores.  \n",
        "   - Unlabeled objects introduce bias, leading to inaccurate learning.  \n",
        "\n",
        "3. **YOLO Compatibility**  \n",
        "   - Requires a specific directory format:  \n",
        "     ```bash\n",
        "     dataset/\n",
        "     ├── train/images/  # .jpg, .png\n",
        "     └── train/labels/  # .txt (one file per image)\n",
        "     ```  \n",
        "   - Class IDs must match the YAML configuration file.  \n",
        "\n",
        "The provided dataset follows the standard structure, with separate training and validation sets, as well as distinct folders for images and labels. Since it includes a YAML file and annotations in `.txt` format structured as:  \n",
        "   - (`class`, `center_x`, `center_y`, `height`, `width`)  \n",
        "   - Values normalized between **0 and 1**  \n",
        "\n",
        "This suggests that the dataset was created using an auxiliary labeling tool, such as [Roboflow](https://roboflow.com), and later converted to YOLO format. By visualizing an image alongside its corresponding annotations, the format was confirmed.**negrito**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcHEFiG3CPWY"
      },
      "outputs": [],
      "source": [
        "def plot_label_check(img, title):\n",
        "    sample_img = cv2.imread(img_path)\n",
        "    annotated_img = sample_img.copy()\n",
        "\n",
        "    h, w = sample_img.shape[:2]\n",
        "    label_path = img_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
        "\n",
        "            x_center, y_center = int(x_center * w), int(y_center * h)\n",
        "            width, height = int(width * w), int(height * h)\n",
        "\n",
        "            x1, y1 = x_center - width // 2, y_center - height // 2\n",
        "            x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "            cv2.rectangle(annotated_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
        "\n",
        "    ax[0].imshow(cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB))\n",
        "    ax[0].set_title(\"Original Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "\n",
        "    ax[1].imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
        "    ax[1].set_title(\"Annotated Image\")\n",
        "    ax[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f\"{title}: {os.path.basename(img_path)}\")\n",
        "    plt.show()\n",
        "\n",
        "img_path = \"datasets/erick/val/images/7126_18_7281_71-6_2ND.jpg\"\n",
        "plot_label_check(img_path, \"Sample Image with Bounding Box\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhDAfDPcCVC6"
      },
      "source": [
        "#2.&nbsp;Image Resolution and Color Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC66xwziCXCv"
      },
      "source": [
        "### **Visualizing Image Resolutions**  \n",
        "\n",
        "Analyzing the distribution of image resolutions in the **training** and **validation** sets helps identify potential imbalances that could affect model training and generalization.  \n",
        "\n",
        "### **Methodology:**  \n",
        "1. **Extract Resolutions:** Collect width and height from all images.  \n",
        "2. **Count Occurrences:** Determine the number of unique resolutions and the number of images per resolution.  \n",
        "\n",
        "### **Observations:**  \n",
        "\n",
        "1. **Training Set Imbalance:**  \n",
        "   - 80% of the training images are concentrated in just **two resolutions** (`4000x2252` and `4032x3024`).  \n",
        "   - Resolutions like `1600x900` and `2000x1126` contain only **one image each**, which could lead to *overfitting* for these specific formats.  \n",
        "\n",
        "2. **Limited Validation Coverage:**  \n",
        "   - The validation set covers only **50%** of the resolutions found in the training set.  \n",
        "   - The resolution `4032x3024` has only **one image** in validation, making evaluation unreliable for this format.  \n",
        "\n",
        "### **Implications:**  \n",
        "- **Resolution Bias:** The model may perform poorly on underrepresented resolutions (e.g., `1600x900`).  \n",
        "- **Preprocessing Needs:** Different resolutions require strategies like *resizing* or *padding* for consistency.  \n",
        "- **Generalization Issues:** The validation set does not sufficiently cover the training resolutions, increasing performance drops in real-world deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAVEegKhCZKW"
      },
      "outputs": [],
      "source": [
        "def get_resolution_and_color_means(image_dir):\n",
        "    resolutions = []\n",
        "    color_means = []\n",
        "\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is not None:\n",
        "            # Get resolution\n",
        "            h, w = img.shape[:2]\n",
        "            resolutions.append((w, h))\n",
        "\n",
        "            # Calculate color means (BGR channels)\n",
        "            color_means.append(img.mean(axis=(0, 1)))\n",
        "    return resolutions, color_means\n",
        "\n",
        "train_image_dir = \"datasets/erick/train/images\"\n",
        "train_label_dir = \"datasets/erick/train/labels\"\n",
        "\n",
        "val_image_dir = \"datasets/erick/val/images\"\n",
        "val_label_dir = \"datasets/erick/val/images\"\n",
        "\n",
        "resolutions_train, color_means_train = get_resolution_and_color_means(train_image_dir)\n",
        "resolutions_val, color_means_val = get_resolution_and_color_means(val_image_dir)\n",
        "\n",
        "print(f\"========= Resolutions =========\")\n",
        "print(f\"Number of unique resolutions (train): {len(set(resolutions_train))}\")\n",
        "print(f\"Images per resolution (train): {dict((res, resolutions_train.count(res)) for res in set(resolutions_train))}\")\n",
        "print(f\"Number of unique resolutions (val): {len(set(resolutions_val))}\")\n",
        "print(f\"Images per resolution (val): {dict((res, resolutions_val.count(res)) for res in set(resolutions_val))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y7p2ROuCe3X"
      },
      "source": [
        "### Analyze Color Channels\n",
        "\n",
        "Comparing the statistical distributions (mean and standard deviation) of the color channel intensities (BGR) between the training and validation sets to ensure consistency and identifying potential biases that may affect the model.\n",
        "\n",
        "### **Methodology:**  \n",
        "1. **Calculation of Statistics:**  \n",
        "   - Mean and standard deviation of the average intensities for each channel (Blue, Green, Red) across all images.  \n",
        "2. **Visualization:**  \n",
        "   - Histograms to compare intensity distributions.  \n",
        "   - Boxplots to analyze variation and outliers.  \n",
        "\n",
        "### **Observations:**\n",
        "The original dataset does not exhibit many outliers and appears consistent with the expected scenario. The high Green and Red values are due to the presence of plants and ground textures, while Blue is typically less prominent in natural environments. As a result, there is no significant indication of bias. However, applying a Hue augmentation could help simulate greater color variability and further enhance the model's robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3UqVqPSCgv0"
      },
      "outputs": [],
      "source": [
        "color_means_train = np.array(color_means_train)\n",
        "color_means_val = np.array(color_means_val)\n",
        "\n",
        "print(f\"========= Color Channel Statistics =========\")\n",
        "print(f\"Color means -> B: {color_means_train[:, 0].mean()}, G: {color_means_train[:, 1].mean()}, R: {color_means_train[:, 2].mean()}\")\n",
        "print(f\"Color std -> B: {color_means_train[:, 0].std()}, G: {color_means_train[:, 1].std()}, R: {color_means_train[:, 2].std()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGHslcCVCjIh"
      },
      "outputs": [],
      "source": [
        "def plot_color_analysis(color_means, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "    # Plot histogram\n",
        "    ax1.hist(color_means[:, 0], bins=50, alpha=0.5, color='b', label='Blue')\n",
        "    ax1.hist(color_means[:, 1], bins=50, alpha=0.5, color='g', label='Green')\n",
        "    ax1.hist(color_means[:, 2], bins=50, alpha=0.5, color='r', label='Red')\n",
        "    ax1.set_title(f'Color Histogram - {title}')\n",
        "    ax1.set_xlabel('Pixel Intensity Mean')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot boxplot\n",
        "    ax2.boxplot([color_means[:, 0], color_means[:, 1], color_means[:, 2]])\n",
        "    ax2.set_title(f'Color Channel Distributions - {title}')\n",
        "    ax2.set_xticks([1, 2, 3])\n",
        "    ax2.set_xticklabels(['Blue', 'Green', 'Red'])\n",
        "    ax2.set_ylabel('Pixel Intensity Mean')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_color_analysis(color_means_train, 'Train Dataset')\n",
        "# plot_color_analysis(color_means_val, 'Validation Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azMM_39TClWg"
      },
      "outputs": [],
      "source": [
        "def analyze_dataset_statistics(image_dir, label_dir):\n",
        "    \"\"\"Analyze dataset statistics including bounding box metrics and class distribution.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing computed statistics\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'bbox_areas': [],\n",
        "        'bbox_aspect_ratios': [],\n",
        "        'class_counts': defaultdict(int),\n",
        "        'num_boxes_per_image': [],\n",
        "        'missing_images': 0,\n",
        "        'corrupted_files': 0\n",
        "    }\n",
        "\n",
        "    label_files = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    print(f\"Analyzing {len(label_files)} label files...\")\n",
        "\n",
        "    for label_file in tqdm(label_files, desc=\"Processing images\"):\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        image_file = os.path.splitext(label_file)[0] + \".jpg\"\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "\n",
        "        # Validate image existence\n",
        "        if not os.path.exists(image_path):\n",
        "            stats['missing_images'] += 1\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            stats['corrupted_files'] += 1\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        num_boxes = 0\n",
        "\n",
        "        # Process labels\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                class_id, x_center, y_center, width, height = map(float, line.split())\n",
        "                stats['class_counts'][int(class_id)] += 1\n",
        "\n",
        "                # Convert to absolute coordinates\n",
        "                width_abs = width * h\n",
        "                height_abs = height * w\n",
        "\n",
        "                # Calculate metrics\n",
        "                area = width_abs * height_abs\n",
        "                aspect_ratio = width_abs / height_abs if height_abs > 0 else 0\n",
        "\n",
        "                stats['bbox_areas'].append(area)\n",
        "                stats['bbox_aspect_ratios'].append(aspect_ratio)\n",
        "                num_boxes += 1\n",
        "\n",
        "        stats['num_boxes_per_image'].append(num_boxes)\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_statistics(stats):\n",
        "    \"\"\"Print formatted statistics\"\"\"\n",
        "    total_images = len(stats['num_boxes_per_image'])\n",
        "    total_boxes = sum(stats['class_counts'].values())\n",
        "\n",
        "    print(\"\\nDataset Statistics Report\")\n",
        "    print(\"============================\")\n",
        "    print(f\"Total images analyzed: {total_images:,}\")\n",
        "    print(f\"Total bounding boxes: {total_boxes:,}\")\n",
        "    print(f\"Missing images: {stats['missing_images']}\")\n",
        "    print(f\"files: {stats['corrupted_files']}\\n\")\n",
        "\n",
        "    print(\"Bounding Box Metrics\")\n",
        "    print(f\"- Average boxes per image: {np.mean(stats['num_boxes_per_image']):.2f}\")\n",
        "    print(f\"- Area (pixels²) - Mean: {np.mean(stats['bbox_areas']):,.2f}, Median: {np.median(stats['bbox_areas']):,.2f}\")\n",
        "    print(f\"- Aspect Ratio - Mean: {np.mean(stats['bbox_aspect_ratios']):.2f}, Median: {np.median(stats['bbox_aspect_ratios']):.2f}\\n\")\n",
        "\n",
        "    print(\"Class Distribution\")\n",
        "    for class_id, count in sorted(stats['class_counts'].items()):\n",
        "        print(f\"  Class {class_id}: {count:,} boxes ({count/total_boxes:.1%})\")\n",
        "\n",
        "def plot_distributions(stats):\n",
        "    \"\"\"Visualize dataset distributions\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Bbox Area\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(stats['bbox_areas'], bins=30, color='skyblue', edgecolor='black')\n",
        "    plt.title('Bounding Box Area Distribution')\n",
        "    plt.xlabel('Pixels²')\n",
        "    plt.ylabel('Count')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    # Aspect Ratio\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(stats['bbox_aspect_ratios'], bins=30, color='salmon', edgecolor='black')\n",
        "    plt.title('Aspect Ratio Distribution')\n",
        "    plt.xlabel('Width/Height Ratio')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "train_stats = analyze_dataset_statistics(train_image_dir, train_label_dir)\n",
        "print_statistics(train_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQGKCFy4Cr_x"
      },
      "source": [
        "#3.&nbsp;Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QzDUgatCsvX"
      },
      "source": [
        "### Why YOLO?\n",
        "\n",
        "The YOLO architecture was selected for its:\n",
        "\n",
        "- Real-time inference capabilities\n",
        "- Balance between accuracy and speed\n",
        "- Strong performance on small datasets\n",
        "- Extensive pre-trained weights availability\n",
        "- Dataset Compatibility\n",
        "- Agricultural Applications [[1](https://www.sciencedirect.com/science/article/pii/S2772375524002533?via%3Dihub), [2](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5090169), [3](https://onlinelibrary.wiley.com/doi/10.1155/2023/4039179), [4](https://www.mdpi.com/2073-4395/14/10/2194), [5](https://www.mdpi.com/2073-4395/14/8/1628)]\n",
        "- SAHI compatibility        [[1](https://ieeexplore.ieee.org/document/9897990), [2](https://github.com/obss/sahi?tab=readme-ov-file)]\n",
        "- Augmentation capabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oposNKGfDEx0"
      },
      "source": [
        "\n",
        "## Training Parameters\n",
        "With the model defined, the training starts! First, there are a few important parameters to decide on.\n",
        "\n",
        "**Model architecture & size (`model`):**\n",
        "\n",
        "Several YOLO versions exists, like yolov5, yolov8 and yolov11 with different models sizes (`yolov8n.pt`, `yolo8m.pt`, `yolo11n.pt`, `yolo11m.pt`, `yolo11x.pt`). Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. For this project the `yolov8n.pt` was chosen since it has less params than older models and provides balanced trade-off between speed and accuracy compared to others models like v9 (more computational heavy) and YOLO11 with is optimized for speed and efficiency, is not the best choice for this task since its focus is on detection smaller objects in higher resolution imagens. The decision to use the `n` variant was made to minimize/prevent overfitting during the transfer learning while ensuring that the model remains sufficiently capable of handling the complexity of small object detection.\n",
        "\n",
        "**Model architecture & size (`model`):**  \n",
        "\n",
        "Several YOLO versions exist, such as YOLOv5, YOLOv8, and YOLOv11, each offering models of varying sizes (`yolov8n.pt`, `yolov8m.pt`, `yolo11n.pt`, `yolo11m.pt`, `yolo11x.pt`). Larger models tend to run slower but offer higher accuracy, while smaller models run faster but sacrifice some accuracy. For this project, the `yolov8n.pt` model was selected. Although it has fewer parameters than the newer models (except YOLOv11), YOLOv8 provides a balanced trade-off between speed and accuracy. YOLOv11, while optimized for speed and efficiency, is not the best choice for this task since its focus is on maximizing performance in real-time scenarios, which is not the primary requirement here. YOLOv8, on the other hand, is well-suited for achieving good detection accuracy with reasonable processing time, making it ideal for the dataset and the computational constraints of this project.\n",
        "\n",
        "\n",
        "**Number of epochs (`epochs`) and (`batchs`)**\n",
        "\n",
        "With a limited dataset, an initial setting of 50 epochs is used to provide sufficient training while mitigating the risk of overfitting. YOLO automatically adjusts the batch size based on GPU memory, typically utilizing around 60% of the GPU’s RAM. For a resolution of 640x640, this usually results in a batch size between 8 and 16. This configuration strikes a balance between efficient training and resource utilization.\n",
        "\n",
        "\n",
        "**Resolution (`imgsz`)**\n",
        "\n",
        "Image Resolution has a large impact on the speed and accuracy of the mode. Lower resolution model will have higher speed but less accuracy. YOLO models are typically trained and inferenced at a 640x640 resolution.\n",
        "\n",
        "\n",
        "**Data Augmentation**\n",
        "\n",
        "Given the limited size of our dataset, we apply data augmentation to improve the model's generalization and robustness. YOLO provides built-in augmentation tools that eliminate the need to create entirely new datasets.\n",
        "The following augmentation techniques are in use:\n",
        "- Mosaic (1.0) – Set to 1.0 to ensure that all images will have that augmentation. This augmentation combines multiple images into one, helping the model learn object variations and different backgrounds.\n",
        "- MixUp (0.5) – Set to 0.5 to ensure that this augmentation only happens 50% of the time. This augmentation blends two images and their labels, improving the model's ability to handle occlusions and uncertainties reducing overfitting.\n",
        "- Scale (0.5) – Randomly resizes objects to enhance scale invariance. (50% probability to scale)\n",
        "- Flip (fliplr: 0.5, flipud: 0.2) – Applies horizontal and vertical flips to introduce viewpoint variations, horizontal is particularly effective for overhead/aerial images like those in our datase. (Aplied to 50% and 20% of images)\n",
        "- HSV Augmentation (hsv_h: 0.015, hsv_s: 0.9, hsv_v: 0.9) – Slightly hye Adjust to prevent extreme color shift, high saturation, and brightness to improve color invariance.\n",
        "\n",
        "\n",
        "**Transfer Learning**\n",
        "\n",
        "Using a pre-trained model leverages knowledge acquired from large datasets, facilitating the extraction of relevant features even if the target class isn't present in the original model. Freezing some of the initial layers preserves fundamental features, while the upper layers are fine-tuned to learn the new classes and adapt to our specific domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ewzNT89DMv4"
      },
      "source": [
        "### Training\n",
        "Run the following code block to begin training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D4DuyipDOix"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "data_yaml_path = \"/content/datasets/erick/data.yaml\"\n",
        "\n",
        "config = {\n",
        "    \"data\": data_yaml_path,\n",
        "    \"epochs\": 50,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch\": 16,\n",
        "    \"workers\": 8,      # Number of CPU workers (default: 8)\n",
        "    \"device\": device,\n",
        "    \"freeze\": list(range(11)), # 0 ... 10\n",
        "    \"mosaic\": 1.0,\n",
        "    \"mixup\": 0.5,\n",
        "    \"scale\": 0.5,\n",
        "    \"fliplr\": 0.5,\n",
        "    \"flipud\": 0.2,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.9,\n",
        "    \"hsv_v\": 0.9,\n",
        "    \"patience\": 30,     # for early stop if theres no improvement\n",
        "    \"seed\": 42,         # for reproducibility\n",
        "    \"single_cls\": True, # for single class detection\n",
        "    \"project\": \"slots\",\n",
        "    \"name\": \"standard\",\n",
        "}\n",
        "\n",
        "results = model.train(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdJmTH2qDTde"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "To evaluate the model's performance, the metrics **Precision**, **Recall**, **mAP** (Mean Average Precision), and **mAP@50-95** were selected. These metrics are widely used in object detection evaluations and are standard in frameworks like YOLO, as well as in renowned benchmarks such as COCO.\n",
        "\n",
        "**Precision** and **Recall** provide essential insights into the quality of the detections by indicating the proportion of true positives relative to predictions and the model's ability to correctly identify objects. In contrast, **mAP@50-95** assesses the average precision of the model across different IoU (Intersection over Union) thresholds, offering a comprehensive view of detection accuracy at varying levels of overlap between predictions and ground truth objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v03QQl_qDZ5X"
      },
      "outputs": [],
      "source": [
        "best_model = YOLO(f\"/content/{config['project']}/{config['name']}/weights/best.pt\")\n",
        "\n",
        "metrics = best_model.val()\n",
        "print(\"======== Metrics ========\")\n",
        "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
        "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
        "print(f\"Results: {metrics.box.p.mean():.4f}\")\n",
        "print(f\"Recall: {metrics.box.r.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM93EfzpDcVc"
      },
      "source": [
        "## Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEwp_hVADbdT"
      },
      "outputs": [],
      "source": [
        "results_path = Path(f\"{config['project']}/{config['name']}/\")\n",
        "results = cv2.imread(str(results_path / \"results.png\"))\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.imshow(cv2.cvtColor(results, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQ1ocMjDpzn"
      },
      "source": [
        "### Test Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kImWpELzDsMc"
      },
      "outputs": [],
      "source": [
        "sample_img = \"/content/datasets/erick/val/images/7039_14_151-6_2nd.jpg\"\n",
        "results = best_model(sample_img)\n",
        "\n",
        "res_plotted = results[0].plot(line_width=3)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title(\"Detected Objects\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2MbyN4wD07I"
      },
      "source": [
        "# 4.&nbsp;Improvement Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_6RW4mtDvHy"
      },
      "source": [
        "## Slicing Strategy for Improving Small Object Detection\n",
        "\n",
        "The original images had high resolutions (e.g., 4032x3024, 4000x2252), and the objects to be detected were very small relative to the overall image size:  \n",
        "- **Average BBox area:** 18,936.92 pixels²  \n",
        "- **Median BBox area:** 20,804.56 pixels²  \n",
        "\n",
        "For reference, in a typical 4000x2252 image (~9MP):  \n",
        "- The object occupied only **~0.2% of the total image area** (18,936 / 9,000,000 ≈ 0.0021).\n",
        "\n",
        "### Identified Problem  \n",
        "At such high resolutions, the object becomes **nearly imperceptible** to conventional detection models, even when using absolute bounding boxes. This results in:  \n",
        "1. Loss of detail during downsampling (resizing to standard resolutions such as 640x640).  \n",
        "2. Learning difficulties due to the low pixel density of the object.\n",
        "\n",
        "### Implemented Solution  \n",
        "The images were divided into **640x640 pixel patches** using a slicing technique, resulting in:  \n",
        "- **Expansion of the training dataset:** from 16 to 581 images.  \n",
        "- **Benefits:**  \n",
        "  - **Artificial increase in the object's relative resolution:**  \n",
        "    In a 640x640 patch (409,600 pixels²), the object's area now represents **~4.6%** (18,936 / 409,600) — making it 22 times more prominent.  \n",
        "  - Reduced computational complexity, as there are fewer pixels to process per iteration.  \n",
        "  - Generation of pseudo-augmented data that provides contextual variations.\n",
        "\n",
        "### Using SAHI (Slicing Aided Hyper Inference) for Small Objects\n",
        "\n",
        "#### How SAHI Complements YOLO\n",
        "| **Approach**         | **SAHI Advantage**                   | **Limitation of Pure YOLO**      |\n",
        "|----------------------|--------------------------------------|----------------------------------|\n",
        "| **Inference**        | Processes the image in slices        | Processes the entire image       |\n",
        "| **Object Size**      | Detects objects smaller than 50px    | Often misses smaller objects |\n",
        "| **Computational Cost** | Increases processing time by ~20%, but improves mAP@50-95 by ~30% | Faster inference, but with lower precision |\n",
        "\n",
        "This slicing strategy, especially when paired with SAHI, significantly enhances the detection of small objects by increasing their relative size and preserving more details during inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTcY-DzrGKeB"
      },
      "source": [
        "## Data Augmentation with Albumentations\n",
        "\n",
        "The use of the Albumentations library, in conjunction with YOLO augmentations, allows for the generation of a larger and more robust dataset by providing techniques not present in YOLO or with limited configuration options. In this sense, the following augmentations were applied to the pipeline:\n",
        "\n",
        "\n",
        "  ```python\n",
        "    # Augmentation pipeline\n",
        "    aug_transforms = A.Compose([\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.3),\n",
        "        A.HueSaturationValue(p=0.2),\n",
        "        A.RandomGamma(gamma_limit=(60, 115), p=0.3),\n",
        "        A.GaussNoise(p=0.3),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "  ```\n",
        "\n",
        "The levels of brightness, saturation, hue and GaussNoise were kept at their default values and adjusting just the probability to happen to , as they are already tuned for use with YOLO, while the RandomGamma required fine-tuning to better distribute the irregular exposure generated (dark/light). The use of Albumentations allowed for a 3x increase in the dataset size, totaling 1,940 images, with 1,743 for training and 197 for validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0DUhVDmFbIE"
      },
      "source": [
        "## Utils File\n",
        "Run the block bellow to create the utils file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYb7Il1nEr_n"
      },
      "outputs": [],
      "source": [
        "%%writefile utils.py\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import yaml\n",
        "import shutil\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import albumentations as A\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.utils.cv import read_image\n",
        "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
        "\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def load_yaml(yaml_file):\n",
        "    \"\"\"Load YAML file and return contents.\"\"\"\n",
        "    with open(yaml_file, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "def plot_training_results(results_path):\n",
        "    \"\"\"\n",
        "    Load and plot the training results from the specified CSV file.\n",
        "    \"\"\"\n",
        "    results = pd.read_csv(results_path)\n",
        "    print(results.head())\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(results['train/box_loss'], label='train/box_loss')\n",
        "    plt.plot(results['val/box_loss'], label='val/box_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Box Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # mAP plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(results['metrics/mAP50(B)'], label='metrics/mAP50(B)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mAP')\n",
        "    plt.title('mAP')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_color_histogram(color_means, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(color_means[:, 0], bins=50, alpha=0.5, color='b', label='Blue')\n",
        "    plt.hist(color_means[:, 1], bins=50, alpha=0.5, color='g', label='Green')\n",
        "    plt.hist(color_means[:, 2], bins=50, alpha=0.5, color='r', label='Red')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Pixel Intensity Mean')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_color_channel_distribution(color_means, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.boxplot([color_means[:, 0], color_means[:, 1], color_means[:, 2]])\n",
        "    plt.title('Color Channel Distributions (BGR)')\n",
        "    plt.xticks([1, 2, 3], ['Blue', 'Green', 'Red'])\n",
        "    plt.ylabel('Pixel Intensity Mean')\n",
        "    plt.show()\n",
        "\n",
        "def create_sahi_dataset(data_yaml_path, output_dir, slice_size=640, overlap_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create a SAHI dataset from the original dataset.\n",
        "\n",
        "    Args:\n",
        "        data_yaml_path: Path to the data.yaml file\n",
        "        output_dir: Directory to save the augmented dataset\n",
        "        slice_size: Size of the slices\n",
        "        overlap_ratio: Overlap ratio between slices\n",
        "    \"\"\"\n",
        "    data_config = load_yaml(data_yaml_path)\n",
        "\n",
        "    # Create output directories\n",
        "    sahi_train_dir = os.path.join(output_dir, \"train\")\n",
        "    sahi_val_dir = os.path.join(output_dir, \"val\")\n",
        "    sahi_train_images_dir = os.path.join(sahi_train_dir, \"images\")\n",
        "    sahi_train_labels_dir = os.path.join(sahi_train_dir, \"labels\")\n",
        "    sahi_val_images_dir = os.path.join(sahi_val_dir, \"images\")\n",
        "    sahi_val_labels_dir = os.path.join(sahi_val_dir, \"labels\")\n",
        "\n",
        "    os.makedirs(sahi_train_images_dir, exist_ok=True)\n",
        "    os.makedirs(sahi_train_labels_dir, exist_ok=True)\n",
        "    os.makedirs(sahi_val_images_dir, exist_ok=True)\n",
        "    os.makedirs(sahi_val_labels_dir, exist_ok=True)\n",
        "\n",
        "    # Determine dataset structure\n",
        "    dataset_dir = os.path.dirname(data_yaml_path)\n",
        "    train_dir = os.path.join(dataset_dir, \"train\")\n",
        "    val_dir = os.path.join(dataset_dir, \"val\")\n",
        "\n",
        "    # Process training and validation images\n",
        "    process_dataset_split(train_dir, sahi_train_images_dir, sahi_train_labels_dir, slice_size, overlap_ratio)\n",
        "    process_dataset_split(val_dir, sahi_val_images_dir, sahi_val_labels_dir, slice_size, overlap_ratio)\n",
        "\n",
        "    # Create a new data.yaml\n",
        "    new_data_yaml = os.path.join(output_dir, \"data.yaml\")\n",
        "    data_config['path'] = output_dir\n",
        "    data_config['train'] = os.path.join(output_dir, \"train\", \"images\")\n",
        "    data_config['val'] = os.path.join(output_dir, \"val\", \"images\")\n",
        "\n",
        "    with open(new_data_yaml, 'w') as f:\n",
        "        yaml.dump(data_config, f)\n",
        "\n",
        "    print(f\"SAHI-augmented dataset created at {output_dir}\")\n",
        "    print(f\"New data.yaml file: {new_data_yaml}\")\n",
        "\n",
        "    return new_data_yaml\n",
        "\n",
        "def process_dataset_split(source_dir, output_images_dir, output_labels_dir, slice_size, overlap_ratio):\n",
        "    \"\"\"\n",
        "    Process a dataset split (train or val) with SAHI slicing.\n",
        "\n",
        "    Args:\n",
        "        source_dir: Source directory containing images and labels\n",
        "        output_images_dir: Output directory for images\n",
        "        output_labels_dir: Output directory for labels\n",
        "        slice_size: Size of the slices\n",
        "        overlap_ratio: Overlap ratio between slices\n",
        "    \"\"\"\n",
        "    source_images_dir = os.path.join(source_dir, \"images\")\n",
        "    source_labels_dir = os.path.join(source_dir, \"labels\")\n",
        "\n",
        "    # If directories don't exist, assume images and labels are in the same directory\n",
        "    if not os.path.exists(source_images_dir):\n",
        "        source_images_dir = source_dir\n",
        "        source_labels_dir = source_dir.replace(\"images\", \"labels\")\n",
        "\n",
        "    for img_file in tqdm(os.listdir(source_images_dir), desc=f\"Processing {os.path.basename(source_dir)}\"):\n",
        "        if not (img_file.endswith('.jpg') or img_file.endswith('.png') or img_file.endswith('.jpeg')):\n",
        "            continue\n",
        "\n",
        "        img_path = os.path.join(source_images_dir, img_file)\n",
        "        base_name = os.path.splitext(img_file)[0]\n",
        "        label_file = f\"{base_name}.txt\"\n",
        "        label_path = os.path.join(source_labels_dir, label_file)\n",
        "\n",
        "        shutil.copy(img_path, os.path.join(output_images_dir, img_file))\n",
        "        if os.path.exists(label_path):\n",
        "            shutil.copy(label_path, os.path.join(output_labels_dir, label_file))\n",
        "\n",
        "        # Perform SAHI slicing\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            h, w = image.shape[:2]\n",
        "\n",
        "            if h > slice_size or w > slice_size:  # Only slice if image is larger than slice size\n",
        "                slice_and_save(img_path, label_path, output_images_dir, output_labels_dir,\n",
        "                              slice_size, overlap_ratio, base_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "def slice_and_save(img_path, label_path, output_images_dir, output_labels_dir, slice_size, overlap_ratio, base_name):\n",
        "    \"\"\"\n",
        "    Slice an image and its annotations using SAHI-style slicing and save to output directories.\n",
        "\n",
        "    Args:\n",
        "        img_path: Path to the input image\n",
        "        label_path: Path to the input label (YOLO format)\n",
        "        output_images_dir: Output directory for sliced images\n",
        "        output_labels_dir: Output directory for sliced labels\n",
        "        slice_size: Size of the slices\n",
        "        overlap_ratio: Overlap ratio between slices\n",
        "        base_name: Base name for the output files\n",
        "    \"\"\"\n",
        "    image = cv2.imread(img_path)\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Get YOLO labels -> [class_id, x_center, y_center, width, height]\n",
        "    annotations = []\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                class_id = int(parts[0])\n",
        "                x_center = float(parts[1]) * w\n",
        "                y_center = float(parts[2]) * h\n",
        "                width = float(parts[3]) * w\n",
        "                height = float(parts[4]) * h\n",
        "\n",
        "                # Convert to xmin, ymin, xmax, ymax\n",
        "                xmin = x_center - width / 2\n",
        "                ymin = y_center - height / 2\n",
        "                xmax = x_center + width / 2\n",
        "                ymax = y_center + height / 2\n",
        "\n",
        "                annotations.append({\n",
        "                    'class_id': class_id,\n",
        "                    'xmin': xmin,\n",
        "                    'ymin': ymin,\n",
        "                    'xmax': xmax,\n",
        "                    'ymax': ymax\n",
        "                })\n",
        "\n",
        "    # Calculate the number of slices\n",
        "    stride = int(slice_size * (1 - overlap_ratio))\n",
        "    num_x_slices = max(1, int(np.ceil((w - slice_size) / stride)) + 1)\n",
        "    num_y_slices = max(1, int(np.ceil((h - slice_size) / stride)) + 1)\n",
        "\n",
        "    slice_idx = 0\n",
        "\n",
        "    # Create slices\n",
        "    for y_idx in range(num_y_slices):\n",
        "        for x_idx in range(num_x_slices):\n",
        "            # Calculate coordinates for this slice\n",
        "            x_start = min(x_idx * stride, w - slice_size)\n",
        "            y_start = min(y_idx * stride, h - slice_size)\n",
        "            x_end = x_start + slice_size\n",
        "            y_end = y_start + slice_size\n",
        "\n",
        "            # Handle edge cases\n",
        "            x_start = max(0, x_end - slice_size)\n",
        "            y_start = max(0, y_end - slice_size)\n",
        "            x_end = min(x_start + slice_size, w)\n",
        "            y_end = min(y_start + slice_size, h)\n",
        "\n",
        "            # Extract slice\n",
        "            slice_img = image[y_start:y_end, x_start:x_end]\n",
        "            slice_h, slice_w = slice_img.shape[:2]\n",
        "\n",
        "            # Create labels for the slice\n",
        "            slice_annotations = []\n",
        "            for anno in annotations:\n",
        "                # Check if annotation intersects with the slice\n",
        "                if (anno['xmin'] < x_end and anno['xmax'] > x_start and\n",
        "                    anno['ymin'] < y_end and anno['ymax'] > y_start):\n",
        "\n",
        "                    # Clip the bounding box to the slice\n",
        "                    xmin_clipped = max(0, anno['xmin'] - x_start)\n",
        "                    ymin_clipped = max(0, anno['ymin'] - y_start)\n",
        "                    xmax_clipped = min(slice_w, anno['xmax'] - x_start)\n",
        "                    ymax_clipped = min(slice_h, anno['ymax'] - y_start)\n",
        "\n",
        "                    width_clipped = xmax_clipped - xmin_clipped\n",
        "                    height_clipped = ymax_clipped - ymin_clipped\n",
        "\n",
        "                    # Check if the area is significant\n",
        "                    original_area = (anno['xmax'] - anno['xmin']) * (anno['ymax'] - anno['ymin'])\n",
        "                    clipped_area = width_clipped * height_clipped\n",
        "\n",
        "                    # Only include if the intersection is significant (at least 30% of original bbox is visible)\n",
        "                    if clipped_area >= 0.3 * original_area:\n",
        "                        # Convert back to YOLO format (class_id, x_center, y_center, width, height)\n",
        "                        x_center = (xmin_clipped + xmax_clipped) / (2 * slice_w)\n",
        "                        y_center = (ymin_clipped + ymax_clipped) / (2 * slice_h)\n",
        "                        width = (xmax_clipped - xmin_clipped) / slice_w\n",
        "                        height = (ymax_clipped - ymin_clipped) / slice_h\n",
        "\n",
        "                        slice_annotations.append(f\"{anno['class_id']} {x_center} {y_center} {width} {height}\")\n",
        "\n",
        "            if slice_annotations:\n",
        "                slice_file = f\"{base_name}_slice_{slice_idx}.jpg\"\n",
        "                cv2.imwrite(os.path.join(output_images_dir, slice_file), slice_img)\n",
        "\n",
        "                slice_label_file = f\"{base_name}_slice_{slice_idx}.txt\"\n",
        "                with open(os.path.join(output_labels_dir, slice_label_file), 'w') as f:\n",
        "                    f.write('\\n'.join(slice_annotations))\n",
        "\n",
        "                slice_idx += 1\n",
        "\n",
        "def apply_albumentations_augmentations(data_yaml_path):\n",
        "    \"\"\"\n",
        "    Apply albumentations augmentations to the dataset.\n",
        "\n",
        "    Args:\n",
        "        data_yaml_path: Path to the data.yaml file\n",
        "    \"\"\"\n",
        "    data_config = load_yaml(data_yaml_path)\n",
        "    train_dir = data_config['train']\n",
        "\n",
        "    # If train_dir is a relative path, make it absolute based on the yaml location\n",
        "    if not os.path.isabs(train_dir):\n",
        "        base_dir = os.path.dirname(data_yaml_path)\n",
        "        train_dir = os.path.join(base_dir, train_dir)\n",
        "\n",
        "    images_dir = train_dir\n",
        "    if os.path.isdir(os.path.join(train_dir, \"images\")):\n",
        "        images_dir = os.path.join(train_dir, \"images\")\n",
        "\n",
        "    labels_dir = images_dir.replace(\"images\", \"labels\")\n",
        "\n",
        "    # Augmentation pipeline\n",
        "    aug_transforms = A.Compose([\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.3),\n",
        "        A.HueSaturationValue(p=0.2),\n",
        "        A.RandomGamma(gamma_limit=(60, 115), p=0.3),\n",
        "        A.GaussNoise(p=0.3),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "    images = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    for img_file in tqdm(images, desc=\"Applying augmentations\"):\n",
        "        base_name = os.path.splitext(img_file)[0]\n",
        "        img_path = os.path.join(images_dir, img_file)\n",
        "        label_path = os.path.join(labels_dir, f\"{base_name}.txt\")\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        # Read labels\n",
        "        bboxes = []\n",
        "        class_labels = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    class_id = int(parts[0])\n",
        "                    x_center, y_center, width, height = map(float, parts[1:5])\n",
        "                    bboxes.append([x_center, y_center, width, height])\n",
        "                    class_labels.append(class_id)\n",
        "\n",
        "        if not bboxes:\n",
        "            continue\n",
        "\n",
        "        # Apply augmentations 2 times\n",
        "        for i in range(2):\n",
        "            try:\n",
        "                augmented = aug_transforms(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "                aug_image = augmented['image']\n",
        "                aug_bboxes = augmented['bboxes']\n",
        "                aug_class_labels = augmented['class_labels']\n",
        "\n",
        "                # Save augmented image\n",
        "                aug_img_file = f\"{base_name}_aug_{i}.jpg\"\n",
        "                cv2.imwrite(os.path.join(images_dir, aug_img_file), aug_image)\n",
        "\n",
        "                # Save augmented labels\n",
        "                aug_label_file = f\"{base_name}_aug_{i}.txt\"\n",
        "                with open(os.path.join(labels_dir, aug_label_file), 'w') as f:\n",
        "                    for j, bbox in enumerate(aug_bboxes):\n",
        "                        f.write(f\"{int(aug_class_labels[j])} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error augmenting {img_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "def evaluate_with_sahi(model_path, data_yaml_path, slice_size=640, overlap_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Evaluate the model with SAHI.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the trained model weights\n",
        "        data_yaml_path: Path to the data.yaml file\n",
        "        slice_size: Size of the slices\n",
        "        overlap_ratio: Overlap ratio between slices\n",
        "    \"\"\"\n",
        "    data_config = load_yaml(data_yaml_path)\n",
        "    val_dir = data_config['val']\n",
        "\n",
        "    # If val_dir is a relative path, make it absolute based on the yaml location\n",
        "    if not os.path.isabs(val_dir):\n",
        "        base_dir = os.path.dirname(data_yaml_path)\n",
        "        val_dir = os.path.join(base_dir, val_dir)\n",
        "\n",
        "    # Load the model using SAHI\n",
        "    detection_model = AutoDetectionModel.from_pretrained(\n",
        "        model_type='ultralytics',\n",
        "        model_path=model_path,\n",
        "        confidence_threshold=0.3,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Get validation images\n",
        "    if os.path.isdir(os.path.join(val_dir, \"images\")):\n",
        "        val_dir = os.path.join(val_dir, \"images\")\n",
        "\n",
        "    images = [os.path.join(val_dir, f) for f in os.listdir(val_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Evaluate with SAHI\n",
        "    for img_path in tqdm(images, desc=\"Evaluating with SAHI\"):\n",
        "        image = read_image(img_path)\n",
        "        img_name = os.path.basename(img_path).split('.')[0]\n",
        "\n",
        "        standard_prediction = get_prediction(\n",
        "            image=image,\n",
        "            detection_model=detection_model,\n",
        "        )\n",
        "        standard_prediction.export_visuals(export_dir=\"out/standard/\", file_name=img_name, rect_th=2)\n",
        "\n",
        "        sliced_prediction = get_sliced_prediction(\n",
        "            image=image,\n",
        "            detection_model=detection_model,\n",
        "            slice_height=slice_size,\n",
        "            slice_width=slice_size,\n",
        "            overlap_height_ratio=overlap_ratio,\n",
        "            overlap_width_ratio=overlap_ratio\n",
        "        )\n",
        "\n",
        "        print(f\"\\nImage: {img_name}\")\n",
        "        print(f\"Standard prediction count: {len(standard_prediction.object_prediction_list)}\")\n",
        "        print(f\"SAHI sliced prediction count: {len(sliced_prediction.object_prediction_list)}\")\n",
        "        sliced_prediction.export_visuals(export_dir=\"out/sahi/\", file_name=img_name, rect_th=2)\n",
        "\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"SAHI implementation for YOLO with small dataset\")\n",
        "    parser.add_argument('--data', type=str, default='datasets/erick/data.yaml',\n",
        "                        help='Path to data.yaml file')\n",
        "    parser.add_argument('--output', type=str, default='datasets/slots',\n",
        "                        help='Output directory for the sliced SAHI dataset')\n",
        "    parser.add_argument('--slice-size', type=int, default=640,\n",
        "                        help='Size of the slices')\n",
        "    parser.add_argument('--overlap-ratio', type=float, default=0.2,\n",
        "                        help='Overlap ratio between slices')\n",
        "    parser.add_argument('--skip-slicing', action='store_true',\n",
        "                        help='Skip SAHI slicing')\n",
        "    parser.add_argument('--skip-augmentations', action='store_true',\n",
        "                        help='Skip Albumentations augmentations')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create sliced (SAHI) dataset\n",
        "    if not args.skip_slicing:\n",
        "        data_yaml_path = create_sahi_dataset(\n",
        "            args.data,\n",
        "            args.output,\n",
        "            args.slice_size,\n",
        "            args.overlap_ratio\n",
        "        )\n",
        "    else:\n",
        "        data_yaml_path = args.data\n",
        "\n",
        "    if not args.skip_augmentations:\n",
        "        print(\"Applying additional augmentations...\")\n",
        "        apply_albumentations_augmentations(data_yaml_path)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    # evaluate_with_sahi(r\"C://Users/rocha/Documents/NACRE/sahi_improved_yolo/yolov8n_mixed_resolution/weights/best.pt\", r\"C://Users/rocha/Documents/NACRE/datasets/erick/data.yaml\", 640, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIccitc6Ey-J"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "\n",
        "output_dir = \"/content/datasets/erick_sahi_augmented\"\n",
        "new_data_yaml = utils.create_sahi_dataset(data_yaml_path, output_dir)\n",
        "# utils.apply_albumentations_augmentations(\"C://Users/rocha/Documents/NACRE/datasets/erick_sahi_augmented/data.yaml\")\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "config = {\n",
        "    \"data\": f\"/content/{new_data_yaml}\",\n",
        "    \"epochs\": 20,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch\": 8,\n",
        "    \"workers\": 0,\n",
        "    \"device\": device,\n",
        "    \"freeze\": list(range(11)), # 0 ... 10\n",
        "    \"augment\": True,\n",
        "    \"mosaic\": 1.0,\n",
        "    \"mixup\": 0.5,\n",
        "    \"scale\": 0.5,\n",
        "    \"fliplr\": 0.5,\n",
        "    \"flipud\": 0.2,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.9,\n",
        "    \"hsv_v\": 0.9,\n",
        "    \"patience\": 30,\n",
        "    \"seed\": 42,\n",
        "    \"single_cls\": True,\n",
        "    \"project\": \"slots\",\n",
        "    \"name\": \"sahi\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFRAvyweEwvo"
      },
      "source": [
        "## Training\n",
        "\n",
        "Run the block bellow to train the new version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0OCjptJbNYg"
      },
      "source": [
        "#5.&nbsp;Results Discution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdLHmifkE51z"
      },
      "outputs": [],
      "source": [
        "results = model.train(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz-T75lDE9zJ"
      },
      "source": [
        "As shown in the figure below, which presents the model metrics, the training losses (`box_loss`, `cls_loss`, and `df_loss`) consistently decrease over the epochs, indicating that the model is effectively learning to locate and classify objects. The validation losses (`val/box_loss`, `val/cls_loss`, `val/df_loss`) also decline, albeit at a slightly slower pace, suggesting good generalization.\n",
        "\n",
        "Meanwhile, the performance metrics (`Precision`, `Recall`, `mAP@50`, `mAP@50-95`) progressively increase and stabilize at high values. This indicates that the model is improving in terms of precision (correct predictions). However, the `mAP@50-95` still shows a drop, which suggest difficulties in precisely localizing objects at higher IoU thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67Uy8II1E_Wv"
      },
      "outputs": [],
      "source": [
        "results_path = Path(f\"/content/{config['project']}/{config['name']}/\")\n",
        "results = cv2.imread(str(results_path / \"results.png\"))\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.imshow(cv2.cvtColor(results, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# slicing over the original val (5 images - full size)\n",
        "utils.evaluate_with_sahi(f\"/content/{config['project']}/{config['name']}/weights/best.pt\", data_yaml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVvoM78KFIP9"
      },
      "source": [
        "As can be seen in the figure bellow (after running the block), applying SAHI significantly improved class identification accuracy, increasing from approximately 50% to 80%. Additionally, detections became more consistent, reducing false positives and further demonstrating the effectiveness of the technique. However, it was observed that images with better lighting conditions yielded more stable results, suggesting that illumination plays a crucial role in detection performance and can be further improved.\n",
        "\n",
        "## Further Improvement Sugestions\n",
        "1. **Data Improvements**:\n",
        "   - Increase dataset size\n",
        "   - Add more challenging scenarios (occlusions, varying lighting)\n",
        "   - Improve annotation quality to reduce label incosistence\n",
        "2. **Model Improvements**:\n",
        "   - Larger YOLO variants (yolov8s/yolov8x/yolo11s/yolo11l)\n",
        "   - Explore R-CNN models for higher accuracy (like detectron2)\n",
        "3. **Training Improvements**:\n",
        "   - Hyperparameter optimization (learning rate, weight decay)\n",
        "   - Tunning with a wide space range (Computational Heavy)\n",
        "   - Longer training with progressive resizing\n",
        "4. **Post-processing**:\n",
        "   - Optimize confidence thresholds to reduce false positives\n",
        "   - Implement/Use test-time augmentation (TTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T_BYAfXFFnB"
      },
      "outputs": [],
      "source": [
        "# Load the SAHI results\n",
        "sahi_image_paths = glob.glob(os.path.join(\"out/sahi/\", \"*.*\"))\n",
        "sahi_image_paths = [p for p in sahi_image_paths if p.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "sahi_img = cv2.imread(sahi_image_paths[0])\n",
        "\n",
        "# Load the standard YOLO results\n",
        "stantard_image_paths = glob.glob(os.path.join(\"out/standard/\", \"*.*\"))\n",
        "stantard_image_paths = [p for p in stantard_image_paths if p.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "stantard_img = cv2.imread(stantard_image_paths[0])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "axes[0].imshow(cv2.cvtColor(sahi_img, cv2.COLOR_BGR2RGB))\n",
        "axes[0].set_title(f\"SAHI: {os.path.basename(sahi_image_paths[0])}\", fontsize=12)\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(cv2.cvtColor(stantard_img, cv2.COLOR_BGR2RGB))\n",
        "axes[1].set_title(f\"Standard YOLO: {os.path.basename(stantard_image_paths[0])}\", fontsize=12)\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(f\"SAHI vs Standard YOLO Inference Comparison\", y=0.98)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
